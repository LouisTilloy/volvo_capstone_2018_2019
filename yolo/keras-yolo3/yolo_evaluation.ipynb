{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualization YOLO efficiency </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to have run the following commands before running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python pre_process_lisa.py\n",
    "# (if that does not work, run this command line inside a terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also be sure to have the following folder architecture:\n",
    "- LISA_TS/\n",
    "    - aiua120214-0/\n",
    "    - aiua120214-1/\n",
    "    - ...\n",
    "    - readme.txt\n",
    "    - videoSources.txt\n",
    "    \n",
    "    \n",
    "- LISA_TS_extension/\n",
    "    - 2014-04-24_10-59/\n",
    "    - 2014-04-24_11-43/\n",
    "    - ...\n",
    "    - 2014-07-11_13-47/\n",
    "    - allTrainingAnnotations.csv\n",
    "    \n",
    "    \n",
    "- weights/\n",
    "    - trained_weights_final.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(To obtain *trained_weights_final.h5* either:\n",
    "- train the network by running *python train.py* (follow the readme instructions for more information, it needs a GPU and takes ~2 days for 150 epochs)\n",
    "- or ask Louis for the last trained weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loss evolution in training</h2>(On the training dataset and the validation dataset)\n",
    "\n",
    "abscisse units: epoch\n",
    "screenshots from tensorflow\n",
    "(last update 03/06/2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td> <img src=\"screenshots/loss.png\" width=500> </td>\n",
    "    <td> <img src=\"screenshots/val_loss.png\" width=500> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Accuracy computing </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/002/trained_weights_final.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "from eval_utils import YOLOPlus\n",
    "\n",
    "model_path = \"logs/002/trained_weights_final.h5\"\n",
    "classes_path = \"model_data/lisa_classes.txt\"\n",
    "\n",
    "yolo = YOLOPlus(model_path=model_path, classes_path=classes_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Accuracy on train + validation dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import load_data, load_classes\n",
    "\n",
    "train_dict, train_imgs = load_data(\"train_lisa.txt\")\n",
    "classes = load_classes(\"lisa_classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90229a445c584fea9d64a89de22093e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7940), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average accuracy (train):  96.17 %\n"
     ]
    }
   ],
   "source": [
    "from eval_utils import detect_img, prediction_not_ok\n",
    "\n",
    "n_examples = len(train_imgs)\n",
    "indices = np.random.choice(len(train_imgs), n_examples, replace=False)\n",
    "\n",
    "n_good_predictions = 0\n",
    "wrong_images_paths = []\n",
    "mistakes_type = []\n",
    "for input_path in tqdm(np.array(train_imgs)[indices]):\n",
    "    r_image, labels, scores, boxes = detect_img(input_path, yolo, score_threshold=0.4)\n",
    "    \n",
    "    true_infos = train_dict[input_path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    \n",
    "    whole_pred_ok = True\n",
    "    for label, score, box in zip(labels, scores, boxes):\n",
    "        local_pred_ok = False\n",
    "        for true_box, true_label in zip(true_boxes, true_labels):\n",
    "            local_pred_ok = local_pred_ok or\\\n",
    "                    not prediction_not_ok(score, true_label, true_box, label, box)\n",
    "        whole_pred_ok = whole_pred_ok and local_pred_ok\n",
    "    n_good_predictions += whole_pred_ok\n",
    "    \n",
    "    \n",
    "    if not whole_pred_ok:\n",
    "        # *** Uncomment this line to display the wrong predictions ***\n",
    "        #r_image.show()\n",
    "        wrong_images_paths.append(input_path)\n",
    "        \n",
    "\n",
    "print(\"average accuracy (train): \", round((n_good_predictions * 100)/n_examples, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg accuracy: 96.17%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True boxes:  [[758, 134, 801, 179]]\n",
      "True labels:  ['stop']\n",
      "boxes:  [(568, 188, 581, 209), (756, 132, 803, 176)]\n",
      "labels:  ['speedLimit30', 'stop']\n",
      "scores:  [0.81308377, 0.9998853]\n",
      "\n",
      "True boxes:  [[472, 294, 488, 309]]\n",
      "True labels:  ['stop']\n",
      "boxes:  [(468, 290, 484, 305)]\n",
      "labels:  ['stop']\n",
      "scores:  [0.93562055]\n",
      "\n",
      "True boxes:  [[448, 32, 478, 59]]\n",
      "True labels:  ['stop']\n",
      "boxes:  [(0, 42, 14, 65), (446, 31, 477, 59)]\n",
      "labels:  ['stop', 'stop']\n",
      "scores:  [0.92357296, 0.99991727]\n",
      "\n",
      "True boxes:  [[31, 434, 64, 479], [958, 390, 1001, 452]]\n",
      "True labels:  ['speedLimit45', 'speedLimit45']\n",
      "boxes:  [(34, 433, 67, 481), (31, 433, 62, 480), (955, 390, 997, 446)]\n",
      "labels:  ['speedLimit50', 'speedLimit45', 'speedLimit45']\n",
      "scores:  [0.4906777, 0.8637572, 0.9930053]\n",
      "\n",
      "True boxes:  [[614, 182, 634, 201], [279, 195, 294, 213]]\n",
      "True labels:  ['stop', 'doNotEnter']\n",
      "boxes:  [(612, 178, 628, 197)]\n",
      "labels:  ['stop']\n",
      "scores:  [0.52497995]\n",
      "\n",
      "True boxes:  [[434, 197, 455, 221]]\n",
      "True labels:  ['speedLimit25']\n",
      "boxes:  [(433, 194, 455, 219), (93, 224, 106, 242)]\n",
      "labels:  ['speedLimit25', 'speedLimitUrdbl']\n",
      "scores:  [0.9413937, 0.95902914]\n",
      "\n",
      "True boxes:  [[440, 446, 465, 481]]\n",
      "True labels:  ['speedLimit25']\n",
      "boxes:  [(329, 43, 383, 109), (437, 445, 460, 484)]\n",
      "labels:  ['noUTurn', 'speedLimit25']\n",
      "scores:  [0.5548198, 0.99811304]\n",
      "\n",
      "True boxes:  [[697, 229, 754, 283]]\n",
      "True labels:  ['intersectionLaneControl']\n",
      "boxes:  [(22, 206, 54, 243), (696, 219, 752, 281)]\n",
      "labels:  ['intersectionLaneControl', 'intersectionLaneControl']\n",
      "scores:  [0.41700912, 0.9712098]\n",
      "\n",
      "True boxes:  [[668, 36, 701, 65]]\n",
      "True labels:  ['yield']\n",
      "boxes:  [(667, 28, 697, 64), (667, 28, 697, 64)]\n",
      "labels:  ['yieldAhead', 'yield']\n",
      "scores:  [0.48867294, 0.95514035]\n",
      "\n",
      "True boxes:  [[484, 205, 499, 220], [135, 206, 152, 223]]\n",
      "True labels:  ['stop', 'stop']\n",
      "boxes:  [(145, 205, 155, 223), (479, 195, 493, 218)]\n",
      "labels:  ['stop', 'stop']\n",
      "scores:  [0.43028754, 0.53940606]\n",
      "\n",
      "True boxes:  [[168, 62, 292, 190]]\n",
      "True labels:  ['thruMergeRight']\n",
      "boxes:  [(167, 45, 288, 191)]\n",
      "labels:  ['addedLane']\n",
      "scores:  [0.40037122]\n",
      "\n",
      "True boxes:  [[831, 451, 898, 515], [846, 414, 884, 457]]\n",
      "True labels:  ['speedLimit15', 'pedestrianCrossing']\n",
      "boxes:  [(837, 448, 895, 513), (838, 446, 893, 507), (851, 416, 882, 453)]\n",
      "labels:  ['speedLimit15', 'pedestrianCrossing', 'pedestrianCrossing']\n",
      "scores:  [0.70127714, 0.86928993, 0.8961139]\n",
      "\n",
      "True boxes:  [[820, 149, 863, 202]]\n",
      "True labels:  ['rampSpeedAdvisory20']\n",
      "boxes:  [(827, 145, 857, 194), (824, 148, 860, 198)]\n",
      "labels:  ['rampSpeedAdvisory20', 'truckSpeedLimit55']\n",
      "scores:  [0.5485336, 0.75390786]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Little exploration of the mistakes\n",
    "import time\n",
    "\n",
    "for image_path in wrong_images_paths:\n",
    "    r_image, labels, scores, boxes = detect_img(image_path, yolo, 0.4)\n",
    "    true_infos = train_dict[image_path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    print(\"True boxes: \", true_boxes)\n",
    "    print(\"True labels: \", [classes[label] for label in true_labels])\n",
    "    print(\"boxes: \", boxes)\n",
    "    print(\"labels: \", [classes[label] for label in labels])\n",
    "    print(\"scores: \", scores)\n",
    "    print()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    r_image.show()\n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average accuracy (train): \", round((n_good_predictions * 100)/n_examples, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Accuracy on extension dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict, test_imgs = load_data(\"test_lisa.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e53bad642874c4aaca5e72167618af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1984), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average accuracy (train):  96.22 %\n"
     ]
    }
   ],
   "source": [
    "from eval_utils import detect_img, prediction_not_ok\n",
    "\n",
    "n_examples = len(test_imgs)\n",
    "indices = np.random.choice(len(test_imgs), n_examples, replace=False)\n",
    "\n",
    "n_good_predictions = 0\n",
    "wrong_images_paths_test = []\n",
    "mistakes_type = []\n",
    "for input_path in tqdm(np.array(test_imgs)[indices]):\n",
    "    r_image, labels, scores, boxes = detect_img(input_path, yolo, score_threshold=0.4)\n",
    "    \n",
    "    true_infos = test_dict[input_path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    \n",
    "    whole_pred_ok = True\n",
    "    for label, score, box in zip(labels, scores, boxes):\n",
    "        local_pred_ok = False\n",
    "        for true_box, true_label in zip(true_boxes, true_labels):\n",
    "            local_pred_ok = local_pred_ok or\\\n",
    "                    not prediction_not_ok(score, true_label, true_box, label, box)\n",
    "        whole_pred_ok = whole_pred_ok and local_pred_ok\n",
    "    n_good_predictions += whole_pred_ok\n",
    "    \n",
    "    # *** Uncomment these 2 lines to display the wrong predictions ***\n",
    "    if not whole_pred_ok:\n",
    "        #r_image.show()\n",
    "        wrong_images_paths_test.append(input_path)\n",
    "        \n",
    "\n",
    "print(\"average accuracy (train): \", round((n_good_predictions * 100)/n_examples, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg accuracy(test): 96.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wrong_images_paths.txt\", \"w\") as file:\n",
    "    for path in wrong_images_paths:\n",
    "        file.write(path + '\\n')\n",
    "    for path in wrong_images_paths_test[:-1]:\n",
    "        file.write(path + '\\n')\n",
    "    file.write(wrong_images_paths_test[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store images in a 'mistakes' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1ad7bc9bae41fd8e3f08d0eb01caf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=304), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5923ed2983c84c2a8fb850d90bf197e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To save the mistakes into their corresponding folders\n",
    "from eval_utils import IoU\n",
    "\n",
    "def save_img_if_mistake(img_path, r_image, labels, boxes, true_labels, true_boxes):\n",
    "    \"\"\"\n",
    "    If YOLO made a mistake on this image,\n",
    "    it saves the image r_image, giving it the\n",
    "    same name as the the name of the original image\n",
    "    \"\"\"\n",
    "    more_or_less_preds_error = len(labels) != len(true_labels)\n",
    "    \n",
    "    iou_error = False\n",
    "    label_error = False\n",
    "    min_iou_flag = False\n",
    "    for label, box in zip(labels, boxes):\n",
    "        for true_label, true_box in zip(true_labels, true_boxes):\n",
    "            if IoU(box, true_box) >= 0.1:\n",
    "                min_iou_flag = True\n",
    "                if label != true_label:\n",
    "                    label_error = True\n",
    "                if IoU(box, true_box) < 0.5:\n",
    "                    iou_error = True\n",
    "    \n",
    "    if label_error:\n",
    "        r_image.save(\"mistakes/label/\" + path.split(\"/\")[-1])\n",
    "    if iou_error:\n",
    "        r_image.save(\"mistakes/iou/\" + path.split(\"/\")[-1])\n",
    "    if more_or_less_preds_error:\n",
    "        r_image.save(\"mistakes/more_or_less_preds/\" + path.split(\"/\")[-1])\n",
    "        \n",
    "\n",
    "for path in tqdm(wrong_images_paths):\n",
    "    r_image, labels, scores, boxes = detect_img(path, yolo, 0.4)\n",
    "    true_infos = train_dict[path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    \n",
    "    save_img_if_mistake(path, r_image, labels, boxes, true_labels, true_boxes)\n",
    "        \n",
    "        \n",
    "for path in tqdm(wrong_images_paths_test):\n",
    "    r_image, labels, scores, boxes = detect_img(path, yolo, 0.4)\n",
    "    true_infos = test_dict[path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    \n",
    "    save_img_if_mistake(path, r_image, labels, boxes, true_labels, true_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average accuracy (test): \", round((n_good_predictions * 100)/n_examples, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the mistakes info into a dictionnary\n",
    "mistakes_info_dict = {}\n",
    "for path in tqdm(wrong_images_paths):\n",
    "    r_image, labels, scores, boxes = detect_img(path, yolo, 0.4)\n",
    "    true_infos = train_dict[path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    \n",
    "    mistakes_info_dict[path]= {\n",
    "        \"true_boxes\": true_boxes,\n",
    "        \"true_labels\": true_labels,\n",
    "        \"boxes\": boxes,\n",
    "        \"labels\": labels,\n",
    "        \"scores\": score)\n",
    "    }\n",
    "\n",
    "for path in tqdm(wrong_images_paths_test):\n",
    "    r_image, labels, scores, boxes = detect_img(path, yolo, 0.4)\n",
    "    true_infos = test_dict[path]\n",
    "    true_boxes = [info[0:4] for info in true_infos]\n",
    "    true_labels = [info[4] for info in true_infos]\n",
    "    \n",
    "    mistakes_info_dict[path]= {\n",
    "        \"true_boxes\": true_boxes,\n",
    "        \"true_labels\": true_labels,\n",
    "        \"boxes\": boxes,\n",
    "        \"labels\": labels,\n",
    "        \"scores\": score)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the dictionnary inside a json\n",
    "import json\n",
    "\n",
    "def dump_mistakes(file_name)\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(mistakes_info_dict, json_file)\n",
    "        \n",
    "dump_mistakes('mistakes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Results summary </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> With test data set = extension data set </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | train + validation | test        |\n",
    "|--------|--------------------|-------------|\n",
    "|Accuracy| **74.0%**          | **48.56%**   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy on the train+validation dataset overall (the original LISA dataset): 74.0%\n",
    "- Accuracy on the test dataset (the extension LISA dataset): 48.56%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> With test data set = 20% of (original dataset + extension dataset) </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | RUN n°| train + validation | test        |\n",
    "|--------|-------|--------------------|-------------|\n",
    "|Accuracy|   1   | **88.44**          | **84.77**   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analysis & comments </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is clearly overfitting the original dataset, interestingly it is overfitting the train and the validation dataset, meaning that the dataset is not diversified enough -> the data augmentation will help a lot, the extension dataset was not used for training so we can try training it on it as well and see the improvement of the accuracy (on the LISA dataset as well as on real world pictures).\n",
    "\n",
    "The main reasons that lower the accuracy are the following:\n",
    "- There are quite some pictures where there are two signs or more (but still only 1 is labelled), YOLO will detect all signs or only some of them, and in the accuracy computation, only the first detected sign is considered\n",
    "- YOLO will sometimes say there is no sign on a picture that contains signs\n",
    "- YOLO will sometimes identify the sign and its position right but predict a wrong close class (like speed limit 35 instead of speed limit 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no obvious way to compute the accuracy for a detection task, a custom function was used to say whether or not a prediction is considered good, it might need to be changed. (An improvement would be to use the mAP metric: https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)\n",
    "\n",
    "These are the criterions to consider a prediction good (Only the first prediction is considered, all others are ignored (there must a smarter way indeed...)):\n",
    "- There must be at least 1 prediction\n",
    "- The predicted label must be the same as the true label\n",
    "- The confidence score must be greater than 0.3\n",
    "- The Intersection over Union of the predicted and ground truth bounding boxes must be greater than 0.5\n",
    "\n",
    "/!\\ The accuracy was only computed for images that actually contain signs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
