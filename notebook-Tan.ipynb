{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EXjzNK-BErOE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Traffic Sign Recognition with Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "gllE7WrRErOH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook is the first part of a tutorial to build a deep learning model for traffic sign recognition. The goal is to build a model that can detect and classify traffic signs in a video stream taken from a moving car. \n",
        "\n",
        "\n",
        "## First Objective: Traffic Sign Classification\n",
        "\n",
        "I'll start with a simple goal: classifiction. Given an image of a traffic sign, our model should be able to tell it's type (e.g. Stop sign, speed limit, yield sign, ...etc.). We'll work with images that are properly cropped such that the traffic sign takes most of the image.\n",
        "\n",
        "\n",
        "For this project, I'm using Pythong 3.5, Tensorflow 0.11, Numpy, Sci-kit Image, and Matplotlib. All pretty standard tools in machine learning. For convenience, I've created a docker image that contains the most common deep learning tools in one place here: https://hub.docker.com/r/waleedka/modern-deep-learning/ . You can run it with this command:\n",
        "\n",
        "```\n",
        "docker run -it -p 8888:8888 -p 6006:6006 -v ~/traffic:/traffic waleedka/modern-deep-learning\n",
        "```\n",
        "\n",
        "Note that I have the files of this project in the ~/traffic directory, and I'm it to /traffic directory in the Docker container. Modify this if you're using a different directory.\n",
        "\n",
        "First step, let's import the needed libraries and get that out of the way."
      ]
    },
    {
      "metadata": {
        "id": "J86eiyM8ErOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import skimage.data\n",
        "import skimage.transform\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Allow image embeding in notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hde2NIy_ErON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Trainging Dataset\n",
        "\n",
        "We're using the Belgian Traffic Sign Dataset. Go to http://btsd.ethz.ch/shareddata/ and download the training and test data. There is a lot of datasets on that page, but you only need the two files listed under **BelgiumTS for Classification (cropped images)**\":\t\n",
        "* BelgiumTSC_Training (171.3MBytes)\n",
        "* BelgiumTSC_Testing (76.5MBytes)\n",
        "\n",
        "After downloading and expanding the files, your directory structure should look something like this:\n",
        "\n",
        "```\n",
        "/traffic/datasets/BelgiumTS/Training/\n",
        "/traffic/datasets/BelgiumTS/Testing/\n",
        "```\n",
        "\n",
        "Each of the two directories above has 62 sub-directories named sequentially from 00000 to 00062. The directory name represents the code (or label) and the images inside the directory are examples of that label. "
      ]
    },
    {
      "metadata": {
        "id": "DfzssOmAErOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parse and Load the Training Data\n",
        "\n",
        "The **Training** directory contains sub-directories with sequental numerical names from 00000 to 00061. The name of the directory represents the labels from 0 to 61, and the images in each directory represent the traffic signs that belong to that label. The images are saved in the not-so-common .ppm format, but luckily, this format is supported in the skimage library."
      ]
    },
    {
      "metadata": {
        "id": "XFJTOFufHigg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cU4t1EvHW0Dz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJQX7Op7W1eS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-YxWwARXEYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lAPJL2p6XI6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Files in Drive:')\n",
        "!ls drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMYeueBDS49l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    \"\"\"Loads a data set and returns two lists:\n",
        "    \n",
        "    images: a list of Numpy arrays, each representing an image.\n",
        "    labels: a list of numbers that represent the images labels.\n",
        "    \"\"\"\n",
        "    # Get all subdirectories of data_dir. Each represents a label.\n",
        "    directories = [d for d in os.listdir(data_dir) \n",
        "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
        "    # Loop through the label directories and collect the data in\n",
        "    # two lists, labels and images.\n",
        "    labels = []\n",
        "    images = []\n",
        "    for d in directories:\n",
        "        label_dir = os.path.join(data_dir, d)\n",
        "        file_names = [os.path.join(label_dir, f) \n",
        "                      for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n",
        "        # For each label, load it's images and add them to the images list.\n",
        "        # And add the label number (i.e. directory name) to the labels list.\n",
        "        for f in file_names:\n",
        "            images.append(skimage.data.imread(f))\n",
        "            labels.append(int(d))\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "# Load training and testing datasets.\n",
        "#ROOT_PATH = \"drive\"\n",
        "train_data_dir = os.path.join(\"drive/VOLVO/BelgiumTSC_Training/Training\")\n",
        "test_data_dir = os.path.join(\"drive/VOLVO/BelgiumTSC_Testing/Testing\")\n",
        "\n",
        "images, labels = load_data(train_data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCHqtpWFGLNy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Produce vendalised images**"
      ]
    },
    {
      "metadata": {
        "id": "hrJGuBSLGG3L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(images)):\n",
        "  images[i] = np.fliplr(images[i])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZitII1PErOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we're loading two lists:\n",
        "* **images** a list of images, each image is represted by a numpy array.\n",
        "* **labels** a list of labels. Integers with values between 0 and 61.\n",
        "\n",
        "\n",
        "It's not usually a good idea to load the whole dataset into memory, but this dataset is small and we're trying to keep the code simple, so it's okay for now. We'll improve it in the next part. For larger datasets, we'd want to have a separate thread loading chunks of data in the background and feeding them to the training thread. "
      ]
    },
    {
      "metadata": {
        "id": "HUhliW7LErOT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore the Dataset\n",
        "\n",
        "How many images and labels do we have?"
      ]
    },
    {
      "metadata": {
        "id": "q1sjIx8bErOV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycFMdTZxErOd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Display the first image of each label."
      ]
    },
    {
      "metadata": {
        "id": "bC3zQw8ZErOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_images_and_labels(images, labels):\n",
        "    \"\"\"Display the first image of each label.\"\"\"\n",
        "    unique_labels = set(labels)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    i = 1\n",
        "    for label in unique_labels:\n",
        "        # Pick the first image for each label.\n",
        "        image = images[labels.index(label)]\n",
        "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
        "        i += 1\n",
        "        _ = plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "display_images_and_labels(images, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mh1Z-RW-ErOi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That looks great! The traffic signs occupy most of the area of each image, which is going to make our job easier: we don't have to look for the sign in the image. And we have a variety of angles and lighting conditions, which will help our model generalize. \n",
        "\n",
        "However, although the images are square-ish, they're not all the same size. They have different aspect ratios. Our simple neural network takes a fixed-size input, so we have a bit of pre-processing to do. We'll get to that soon, but first let's pick a label and see more of it's images. Let's pick label 32:"
      ]
    },
    {
      "metadata": {
        "id": "N6KwLSQgErOk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_label_images(images, label):\n",
        "    \"\"\"Display images of a specific label.\"\"\"\n",
        "    limit = 24  # show a max of 24 images\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    i = 1\n",
        "\n",
        "    start = labels.index(label)\n",
        "    end = start + labels.count(label)\n",
        "    for image in images[start:end][:limit]:\n",
        "        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n",
        "        plt.axis('off')\n",
        "        i += 1\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "display_label_images(images, 21)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_d6Uuh_zErOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Interesting! It looks like our dataset considers all speeding limit signs to be of the same class regardless of the numbers on them. That's fine, as long as we know about it beforehand and don't let it confuse us later when the output doesn't match our expectation. \n",
        "\n",
        "I'll leave exploring other labels as an exercise for you, edit the code above and check other labels. Make sure to check Labels 26 and 27. They also have numbers in a red circle, so our model will have to get really good to differentiate between these 3 classes."
      ]
    },
    {
      "metadata": {
        "id": "AZbsVlusErOp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Handling images of different sizes?\n",
        "\n",
        "Most neural networks expect a fixed-size input, and our network is no exception. But as we've seen above, our images are not all the same size. A common approach is to crop and pad the images to a selected apect ratio, but then we have to make sure that we don't cut-off parts of the traffic signs in the process. That seems like it might require manual work! Let's do a simpler solution instead (a hack really): We'll just resize the images to a fixed size and ignore the distortions caused by the different aspect ratios. A person can easily recognize a traffic sign even if it's compressed or stretched a bit, so we hope that our model can as well. \n",
        "\n",
        "And while we're at it, let's make the images smaller. The larger the input data, the larger the model, and the slower it is to train. In the early stages of development we want fast training to avoid long waits between iterations while we change the code rapidly. \n",
        "\n",
        "What are the sizes of our image anyway?"
      ]
    },
    {
      "metadata": {
        "id": "gsqwPJC6ErOr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for image in images[:5]:\n",
        "    print(\"shape: {0}, min: {1}, max: {2}\".format(image.shape, image.min(), image.max()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0nZYMuOAErOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The sizes seem to hover around 128x128. If we resize them to, say, 32x32, we'll have reduced the data and the model size by a factor of 16. And 32x32 is probably still big enough to recognize the signs, so let's go with that. \n",
        "\n",
        "I'm also in the habit of frequently printing the min() and max() values. It's a simple way to verify the range of your data and catch bugs early."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "JYNF_ui8ErOv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Resize images\n",
        "images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n",
        "                for image in images]\n",
        "display_images_and_labels(images32, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zdex8VirErO0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The 32x32 images are not as sharp but still recognizable. Note that the display above shows the images larger than their real size because the matplotlib library tries to fit them to the grid size. Let's print the sizes of a few images to verify that we got it right."
      ]
    },
    {
      "metadata": {
        "id": "N4NJKPjAErO1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for image in images32[:5]:\n",
        "    print(\"shape: {0}, min: {1}, max: {2}\".format(image.shape, image.min(), image.max()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHM7QBcSErO5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The sizes are correct. But check the min and max values! They now range from 0 to 1.0, which is different from the 0-255 range we saw above. The resizing function did that transformation for us. Normalizing values to the range 0.0-1.0 is very common so we'll keep it. But remember to multiply by 255 if you later want to convert the images back to the normal 0-255 range."
      ]
    },
    {
      "metadata": {
        "id": "QfMRyxczndW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Random crop of images**\n",
        "*Not sure how it works*"
      ]
    },
    {
      "metadata": {
        "id": "ZOCsHsr1nNhf",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "# TensorFlow. 'x' = A placeholder for an image.\n",
        "#original_size = [height, width, channels]\n",
        "#x = tf.placeholder(dtype = tf.float32, shape = original_size)\n",
        "# Use the following commands to perform random crops\n",
        "#crop_size = [new_height, new_width, channels]\n",
        "#seed = np.random.randint(1234)\n",
        "#x = tf.random_crop(x, size = crop_size, seed = seed)\n",
        "#output = tf.images.resize_images(x, size = original_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRFR0lnFErO6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Minimum Viable Model"
      ]
    },
    {
      "metadata": {
        "id": "LstFUcNuErO7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels_a = np.array(labels)\n",
        "images_a = np.array(images32)\n",
        "print(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQs26zuMErPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to create the model.\n",
        "tf.reset_default_graph()\n",
        "def create_model():\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        # Placeholders for inputs and labels.\n",
        "        images_ph = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
        "        labels_ph = tf.placeholder(tf.int32, [None])\n",
        "        dropout_ph = tf.placeholder(tf.float32, ())\n",
        "        \n",
        "        # convolutional layer 1\n",
        "        conv1 = tf.layers.conv2d(\n",
        "            inputs=images_ph,\n",
        "            filters=32,\n",
        "            kernel_size=[5, 5],\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu,\n",
        "            kernel_initializer=tf.initializers.random_uniform(-0.1, 0.1))\n",
        "        \n",
        "        # max pooling layer 1\n",
        "        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
        "        \n",
        "        # convolutional layer 2\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            inputs=pool1,\n",
        "            filters=64,\n",
        "            kernel_size=[3, 3],\n",
        "            padding=\"same\",\n",
        "            activation=tf.nn.relu)\n",
        "        \n",
        "        # max pooling layer 2\n",
        "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
        "        \n",
        "        # Flatten features\n",
        "        images_flat = tf.layers.flatten(pool2)\n",
        "\n",
        "        # Fully connected layer. \n",
        "        logits = tf.contrib.layers.fully_connected(images_flat, 62, activation_fn=None,\n",
        "                                                   weights_initializer=tf.initializers.random_uniform(-0.1, 0.1))\n",
        "        \n",
        "        # dropout layer\n",
        "        logits = tf.nn.dropout(logits, 1 - dropout_ph)\n",
        "\n",
        "        # Convert logits to label indexes (int).\n",
        "        # Shape [None], which is a 1D vector of length == batch_size.\n",
        "        predicted_labels = tf.argmax(logits, 1)\n",
        "\n",
        "        # Define the loss and accuracy function. \n",
        "        # Cross-entropy is a good choice for classification.\n",
        "        loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels_ph))\n",
        "        accuracy = tf.metrics.accuracy(labels_ph, predicted_labels)[1]\n",
        "        \n",
        "        # Create training op.\n",
        "        train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "    return images_ph, labels_ph, dropout_ph, predicted_labels, loss, accuracy, train\n",
        "\n",
        "images_ph, labels_ph, dropout_ph, predicted_labels, loss, accuracy, train = create_model()\n",
        "print(\"loss: \", loss)\n",
        "print(\"predicted_labels: \", predicted_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IIygJ2S7ErPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "-E1fXystErPI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load test data for evaluation while training\n",
        "test_images, test_labels = load_data(test_data_dir)\n",
        "\n",
        "# Transform the images, just like we did with the training set.\n",
        "test_images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n",
        "                 for image in test_images]\n",
        "\n",
        "# Put them into arrays to feed them to tensorflow\n",
        "test_images32, test_labels = np.array(test_images32), np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9FGIlMKErPO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a session to run the graph we created.\n",
        "try:\n",
        "    session = tf.Session()\n",
        "except ValueError:\n",
        "    session.close()\n",
        "    session = tf.Session()\n",
        "\n",
        "# First step is always to initialize all variables. \n",
        "# We don't care about the return value, though. It's None.\n",
        "_ = session.run(tf.global_variables_initializer())\n",
        "\n",
        "# weird case with tensorflow metrics (code from stack overflow to initialize it)\n",
        "running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)\n",
        "session.run(tf.variables_initializer(var_list=running_vars))\n",
        "\n",
        "step=0\n",
        "loss\n",
        "train_loss_list = []\n",
        "train_acc = []\n",
        "test_loss_list = []\n",
        "test_acc = []\n",
        "steps_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ATRqBUF2ErPT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32  # I added batch because gradient descent if faster and it avoids local minimums \n",
        "# (it is a classic machine learning move)\n",
        "n_steps = 20000\n",
        "\n",
        "for i in range(n_steps):\n",
        "    if i==0 or data_index + batch_size > len(labels_a) :\n",
        "        data_index = 0\n",
        "        indices = np.linspace(0, len(labels_a)-1, len(labels_a), dtype=np.int32)\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "    batch_indices = indices[data_index:data_index + batch_size]\n",
        "    session.run(train,\n",
        "                feed_dict={images_ph: images_a[batch_indices],\n",
        "                           labels_ph: labels_a[batch_indices],\n",
        "                           dropout_ph: 0.5})\n",
        "    if i % 1000 == 0:\n",
        "        # Compute train loss and accuracy without dropout on the batch to save some time\n",
        "        train_loss_value, train_accuracy = session.run([loss, accuracy], \n",
        "                            feed_dict={images_ph: images_a[batch_indices],\n",
        "                                       labels_ph: labels_a[batch_indices],\n",
        "                                       dropout_ph: 0.0})\n",
        "        \n",
        "        # Evaluation on the whole test dataset, without dropout\n",
        "        test_loss_value, test_accuracy = session.run([loss, accuracy],\n",
        "                                                     feed_dict={images_ph: test_images32,\n",
        "                                                                labels_ph: test_labels,\n",
        "                                                                dropout_ph: 0.0})\n",
        "        steps_list.append(step)\n",
        "        train_loss_list.append(train_loss_value)\n",
        "        train_acc.append(train_accuracy)\n",
        "        test_loss_list.append(test_loss_value)\n",
        "        test_acc.append(test_accuracy)\n",
        "        \n",
        "        print(\"********** step {} **********\".format(step))\n",
        "        print(\"Train loss: \", train_loss_value)\n",
        "        print(\"Train accuracy: \", train_accuracy)\n",
        "        print(\"Test loss: \", test_loss_value)\n",
        "        print(\"Test accuracy: \", test_accuracy)\n",
        "        \n",
        "    step += 1\n",
        "    data_index += batch_size\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nlfarj9OErPX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "plt.figure()\n",
        "plt.plot(steps_list, train_loss_list, label=\"train loss\")\n",
        "plt.plot(steps_list, test_loss_list, label=\"test loss\")\n",
        "plt.title(\"loss evolution in training\")\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy\n",
        "plt.figure()\n",
        "plt.plot(steps_list, train_acc, label=\"train accuracy\")\n",
        "plt.plot(steps_list, test_acc, label=\"test accuracy\")\n",
        "plt.title(\"accuracy evolution in training\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0XesL3eaErPZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using the Model\n",
        "\n",
        "The session object contains the values of all the variables in our model (i.e. the weights). "
      ]
    },
    {
      "metadata": {
        "id": "pTZMGtM8ErPa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pick 10 random images\n",
        "sample_indexes = random.sample(range(len(images32)), 10)\n",
        "sample_images = [images32[i] for i in sample_indexes]\n",
        "sample_labels = [labels[i] for i in sample_indexes]\n",
        "\n",
        "# Run the \"predicted_labels\" op.\n",
        "predicted = session.run([predicted_labels], \n",
        "                        feed_dict={images_ph: sample_images,\n",
        "                                   dropout_ph: 0.0})[0]\n",
        "print(sample_labels)\n",
        "print(predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crFehr0aErPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display the predictions and the ground truth visually.\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for i in range(len(sample_images)):\n",
        "    truth = sample_labels[i]\n",
        "    prediction = predicted[i]\n",
        "    plt.subplot(5, 2,1+i)\n",
        "    plt.axis('off')\n",
        "    color='green' if truth == prediction else 'red'\n",
        "    plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n",
        "             fontsize=12, color=color)\n",
        "    plt.imshow(sample_images[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T5jj_krWErPj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "It's fun to visualize the results, but we need a more precise way to measure the accuracy of our model. Also, it's important to test it on images that it hasn't seen. And that's where the validation data set comes into play."
      ]
    },
    {
      "metadata": {
        "id": "fX__CWaMErPl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the test dataset.\n",
        "test_images, test_labels = load_data(test_data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nnPpu25kErPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Transform the images, just like we did with the training set.\n",
        "test_images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n",
        "                 for image in test_images]\n",
        "display_images_and_labels(test_images32, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uOWzRGHjErPr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run predictions against the full test set.\n",
        "predicted = session.run([predicted_labels], \n",
        "                        feed_dict={images_ph: test_images32,\n",
        "                                   dropout_ph: 0.0})[0]\n",
        "\n",
        "# Calculate how many matches we got.\n",
        "match_count = sum([int(y == y_) for y, y_ in zip(test_labels, predicted)])\n",
        "accuracy = match_count / len(test_labels)\n",
        "print(\"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOfcJtlinoue",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(test_images32)):\n",
        "    truth = test_labels[i]\n",
        "    prediction = predicted[i]\n",
        "    plt.plot()\n",
        "    plt.axis('off')\n",
        "    if truth != prediction:\n",
        "      fig = plt.figure(figsize=(2, 2))\n",
        "      plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n",
        "             fontsize=16, color=color)\n",
        "      plt.imshow(test_images32[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37CxS6kqnqHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate how many matches we got.\n",
        "match_count = sum([int(y == y_) for y, y_ in zip(test_labels, predicted)])\n",
        "accuracy = match_count / len(test_labels)\n",
        "print(\"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJXifypgErPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Close the session. This will destroy the trained model.\n",
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWHKthEgErP1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}